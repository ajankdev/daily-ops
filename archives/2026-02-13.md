# Daily Digest: 2026-02-13

### Simpler billing, clearer savings: A FinOps guide to updated spend-based CUDs
**Source:** Google Cloud (General)

> **Category:** [GCP_K8S_CORE]
> **Summary:** The article discusses updates to Google Cloud's spend-based Committed Use Discounts (CUDs) model, which aims to simplify billing and provide clearer savings. The new model moves from a credit-based system to a direct discounted price model, making it easier to understand costs and savings. The article also highlights the updated CUD Analysis tool, which offers deeper visibility into CUD coverage and utilization, and provides recommendations for determining commitment levels.
> **Impact:** The updates to the spend-based CUDs model and the CUD Analysis tool can help FinOps teams optimize their cloud spend, simplify their billing process, and make more informed decisions about commitment levels, ultimately leading to cost savings and improved resource utilization.

[Read Article](https://cloud.google.com/blog/topics/cost-management/a-finops-professionals-guide-to-updated-spend-based-cuds/)

---
### v0.16.0
**Source:** vLLM Release

> **Category:** [AI_INFRA]
> **Summary:** The v0.16.0 release of vLLM features significant improvements in performance, hardware support, and API functionality. Key updates include a PyTorch 2.10 upgrade, async scheduling and pipeline parallelism for 30.8% throughput improvement, and a new WebSocket-based Realtime API. Additionally, the release includes support for various hardware platforms such as NVIDIA, AMD ROCm, Intel XPU, and ARM CPU, with optimized performance and quantization.
> **Impact:** This release is expected to have a significant impact on the performance and scalability of vLLM, enabling faster and more efficient processing of large-scale language models. The improved hardware support and optimized performance will also enable better utilization of resources, leading to cost savings and improved overall efficiency.

[Read Article](https://github.com/vllm-project/vllm/releases/tag/v0.16.0)

---
### Gemini 3 Deep Think: Advancing science, research and engineering
**Source:** Google DeepMind

> **Category:** [AI_MODELS]
> **Summary:** The input mentions "Gemini", which is a reference to Google's AI model, indicating an update to its specialized reasoning mode to tackle modern science, research, and engineering challenges.
> **Impact:** This update may improve the accuracy and capabilities of Gemini in solving complex problems, potentially leading to breakthroughs in various fields, and could have a significant impact on operations that rely on AI-driven research and development.

[Read Article](https://deepmind.google/blog/gemini-3-deep-think-advancing-science-research-and-engineering/)

---
### OpenEnv in Practice: Evaluating Tool-Using Agents in Real-World Environments
**Source:** Hugging Face Blog

> **Category:** [AI_MODELS]
> **Summary:** The input title "OpenEnv in Practice: Evaluating Tool-Using Agents in Real-World Environments" suggests a focus on evaluating agents, potentially AI models, in real-world environments, which aligns with the interest area of AI models, including those from Anthropic, OpenAI, Meta, and Google.
> **Impact:** The evaluation of tool-using agents in real-world environments could have significant operational implications, such as improved model performance, adaptability, and decision-making capabilities, potentially leading to more effective and efficient AI model deployments.

[Read Article](https://huggingface.co/blog/openenv-turing)

---
### Why 60% of Java workloads on K8s are wasting resources
**Source:** r/PlatformEngineering

> **Category:** [GCP_K8S_CORE]
> **Summary:** The input discusses Java workloads on Kubernetes (K8s) and how a significant percentage (60%) are wasting resources, implying inefficiencies in deployment, configuration, or management, which could be related to GKE, Anthos, or other K8s-related technologies on GCP.
> **Impact:** Optimizing Java workloads on K8s could lead to significant resource savings and improved performance, directly impacting the operational efficiency and cost-effectiveness of applications deployed on GKE or managed through Anthos, highlighting the need for careful configuration and monitoring.

[Read Article](https://www.reddit.com/r/platformengineering/comments/1r2vtub/why_60_of_java_workloads_on_k8s_are_wasting/)

---
### Hugging Face case study - Google Cloud
**Source:** AI Infra Watch

> **Category:** [AI_INFRA]
> **Summary:** The input is a case study about Hugging Face on Google Cloud, which implies the use of AI infrastructure such as GPU acceleration, scalable computing, and potentially other AI-related services on GCP to support Hugging Face's AI model development and deployment.
> **Impact:** The case study may provide insights into optimizing AI workloads on Google Cloud, potentially leveraging services like NVIDIA GPUs, and could have an operational impact on how AI infrastructure is designed and managed for similar use cases.

[Read Article](https://news.google.com/rss/articles/CBMiY0FVX3lxTFBBS00tUzdyd20xdGd5UTRpV3hVa1FrWW1ieW40bkpMdUo1bGRlUUV0V1hwNl9VM2xvZE9Oc1drejJxYVVzaE91VTBSNmFoMXA0aXMyNUVPdlJvUmJHSVgxVUtsbw?oc=5)

---
### Hugging Face case study - Google Cloud
**Source:** AI Infra Watch

> **Category:** [AI_INFRA]
> **Summary:** The input is a case study about Hugging Face on Google Cloud, which implies the use of AI infrastructure such as GPU acceleration, scalable storage, and potentially other AI-related services like AutoML or AI Platform.
> **Impact:** The case study may provide insights into optimizing AI workloads on Google Cloud, potentially leveraging services like GKE or Anthos to manage and deploy AI models, and could have an impact on the design and implementation of AI infrastructure for similar use cases.

[Read Article](https://news.google.com/rss/articles/CBMiW0FVX3lxTE4wMm8wOThGbU1YTTByb2oyekdmZFdvdnRRX0VmZTdoR1phSk1fR3NOeDRIZ0ZDbDRsRzh2T3B5UWpLSG40UnUyWHB2Y3BqVTE3MWNDUkxQNmVYMW8?oc=5)

---
