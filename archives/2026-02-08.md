# Daily Digest: 2026-02-08

### What actually blocks internal RAG tools from reaching production?
**Source:** r/PlatformEngineering

> **Category:** [AI_INFRA]
> **Summary:** The post discusses the challenges of deploying internal RAG (Retrieve, Augment, Generate) tools, specifically doc-chat tools, to production due to security, compliance, and audit concerns. The author is seeking real-world experiences and examples of actual blockers, such as data leakage, model access, logging, prompt injection, and compliance issues.
> **Impact:** The post highlights the operational challenges of deploying AI-powered tools in a production environment, which can impact the ability to deliver scalable and secure AI solutions, and may require additional investment in infrastructure, security, and compliance measures to address these concerns.

[Read Article](https://www.reddit.com/r/platformengineering/comments/1qym5pr/what_actually_blocks_internal_rag_tools_from/)

---
### Nemo 30B is insane. 1M+ token CTX on one 3090
**Source:** r/LocalLLaMA

> **Category:** [AI_INFRA]
> **Summary:** The user is discussing their experience with running large language models, specifically Nemo 30B, on a single NVIDIA 3090 GPU with 32 GB of RAM. They're impressed by the model's performance, achieving 1M+ token context cache and 35 t/s, making it possible to process long context windows on local hardware.
> **Impact:** The ability to run large language models like Nemo 30B on a single GPU can significantly impact the development and deployment of AI applications, enabling faster and more efficient processing of large datasets and reducing the need for cloud-based infrastructure. This can be particularly useful for applications that require low latency and high throughput, such as real-time language translation or text summarization.

[Read Article](https://www.reddit.com/r/LocalLLaMA/comments/1qy0l26/nemo_30b_is_insane_1m_token_ctx_on_one_3090/)

---
